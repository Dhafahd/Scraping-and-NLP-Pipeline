{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9898a437-f418-421e-b406-d05aa1f764a1",
   "metadata": {},
   "source": [
    "### MST AIDS 2023-2024 (Département Génie Informatique)\n",
    "### Subject : The main purpose behind this lab is to get familiar with Scraping and NLP Pipeline.\n",
    "### Realize by : Chibani Fahd\n",
    "### web sourcest : Aljarida24r\n",
    "### Modèl : NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1fee45-c6a8-416b-a57b-cdaab3bb653a",
   "metadata": {},
   "source": [
    "#### PART 1 : Scraping and storing data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a2df80-de61-4e08-90a0-414b29363c8a",
   "metadata": {},
   "source": [
    "#### I Used Beautiful Soup to scrap multiple pages from Aljarida24  web site and pymongo to connete by Mongodb and store data.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d7bb56-5d22-4495-b2c8-e17da1261567",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# Connect to MongoDB\n",
    "client = MongoClient('mongodb://localhost:27017/')\n",
    "db = client['aljarida24']\n",
    "collection = db['articles']\n",
    "\n",
    "# URL of the main page\n",
    "main_url = 'https://aljarida24.ma/'\n",
    "\n",
    "# Request the main page\n",
    "main_page = requests.get(main_url)\n",
    "\n",
    "# Parse the main page\n",
    "main_soup = BeautifulSoup(main_page.text, 'html.parser')\n",
    "\n",
    "# Find all the links on the main page that contain \"aljarida24.ma/p\"\n",
    "all_links = main_soup.find_all('a', href=True)\n",
    "\n",
    "# Initialize a set to store unique URLs\n",
    "unique_urls = set()\n",
    "\n",
    "# Extract and filter unique URLs from the links\n",
    "for link in all_links:\n",
    "    url = link.get('href')\n",
    "    if url and 'aljarida24.ma/p' in url:\n",
    "        unique_urls.add(url)\n",
    "        if len(unique_urls) >= 100:\n",
    "            break  # Exit loop if we have collected 100 unique URLs\n",
    "\n",
    "# Loop through unique URLs\n",
    "for url in unique_urls:\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "    \n",
    "    para_div = soup.find('div', {'class': 'content-text'})\n",
    "    if para_div:\n",
    "        # Extract header if available\n",
    "        header = soup.find('h1')\n",
    "        if header:\n",
    "            print(\"URL:\", url)\n",
    "            print(\"Header:\", header.text.strip())\n",
    "            header_text = header.text.strip()\n",
    "        \n",
    "        # Initialize a variable to store all paragraphs\n",
    "        all_paragraphs = \"\"\n",
    "\n",
    "        # Extract paragraphs\n",
    "        for item in para_div.contents:\n",
    "            if item.name == 'p':\n",
    "                all_paragraphs += item.text.strip() + \"  \"\n",
    "\n",
    "        print(\"Paragraphs:\", all_paragraphs)\n",
    "        print(\"\\n\")  # Add a new line for separation between articles\n",
    "        # Insert data into MongoDB\n",
    "        article_data = {\n",
    "            \"url\": url,\n",
    "            \"header\": header_text,\n",
    "            \"paragraphs\": all_paragraphs\n",
    "        }\n",
    "        \n",
    "        collection.insert_one(article_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238b916b-f345-47aa-848f-c7ef6393dc58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
